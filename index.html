<!doctype html>

<!-- Any figures to-be called with <img ...> should be placed in /static and called 
as with /static as their root. E.g. <img src="/diagrams/fig1.png">
-->

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="template.v2.js"></script>
  <title>arXiv/viXra - The Data - Garrett Goon</title>
</head>

<body>

  <!-- I added a date field to more easily add the date to the front matter. Removed the DOI fiel -->

  <d-front-matter>
    <script type="text/json">
      {
        "title": "arXiv/viXra - Data",
        "description": "An example project using Webpack, Babel, and Svelte.",
        "authors": [
          {
            "author": "Garrett Goon",
            "authorURL": "https://garrettgoon.com",
            "affiliation": "CMU",
            "affiliationURL": "https://www.cmu.edu/physics/"
          }
        ],
        "katex": {
          "delimiters": [
            {
              "left": "$",
              "right": "$",
              "display": false
            },
            {
              "left": "$$",
              "right": "$$",
              "display": true
            }
          ]
        },
        "date" : "15 December, 2021"
      }
  </script>
  </d-front-matter>

  <d-title>
    <h1>arXiv/viXra - The Data</h1>
    <p>Defining the problem and diving in.</p>
  </d-title>

  <d-article>

    <p>
      <em>
        <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml/tree/main/data_exploration'>Note:
          The Jupyter/Colab notebooks I
          used to perform much of the following analysis can be found on my GitHub page.</a>
      </em>
    </p>

    <h3>Wading Through</h3>

    <blockquote>
      The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly
      inspecting your data.
      This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through
      thousands of examples, understanding their distribution and looking for patterns.
    </blockquote>
    <p>
      This quote is from Andrej Karpaty's excellent post
      <em>
        <a target="_blank" rel="noopener" href="https://arxiv.org">A Recipe for Training Neural Networks
        </a>
      </em>, and it is advice<d-footnote id="d-footnote-wrong-guesses">
        Another sage piece of advice from the post: <em>don't be a hero.</em> Start simple and be conservative in your
        rate of adding bells and whistles.
      </d-footnote> well-taken. Below, I summarize my results in applying this advice to the arXiv/viXra
      datasets. I detail the gross properties of the sets,
      their patterns and distinctions,
      and the process of filtering and normalizing the text. These last two processes serve multiple purposes:
    </p>

    <ul>
      <li>
        <b>Filtering</b>: Removing various types of outliers in the data narrows and helps to define the scope of the
        classification
        problem.
      </li>
      <li>
        <b>Normalizing</b>: Processing the text both prepares the data for ML models and is important for preventing
        accidental cheating via undesired technical clues. A cautionary tale on this last point is provided below.
      </li>
    </ul>

    <p>
      Various design choices enter into the above and I explain the decisions I made below.
    </p>


    <h3>Gross Properties</h3>

    <h4>
      Data Imbalance
    </h4>

    <p>
      There is far more data<d-footnote id='data'>
        The arXiv dataset is <a target='_blank' rel='noopener noreferrer'
          href='https://www.kaggle.com/Cornell-University/arxiv'>
          publicly available on Kaggle.</a>
        I wrote a small <d-code language='python'>python</d-code> web-scraper to collect the viXra data, which can be <a
          target='_blank' rel='noopener noreferrer'
          href='https://www.dropbox.com/s/4gw9wv90kqyeh95/vixra_raw.feather?dl=0'>downloaded here</a> as a <d-code
          language='python'>.feather</d-code> file (18MB).
        <a target='_blank' rel='noopener noreferrer'
          href='https://pandas.pydata.org/docs/reference/api/pandas.read_feather.html'>
          <d-code language='python'>pandas</d-code> handles <d-code language='python'>.feather</d-code> files natively
        </a> and they are a much more efficient alternative to <d-code language='python'>.csv</d-code>.
      </d-footnote> for arXiv than viXra (the ratio of papers is 50:1) and the alignment of categories is not perfect.
      That is, while the two repositories cover many of the same topics, such as
    </p>
    <ul>
      <li>
        Number Theory
      </li>
      <li>
        Condensed Matter
      </li>
      <li>
        Data Structures and Algorithms
      </li>
    </ul>
    <p>
      there also exist categories that belong to arXiv or viXra alone:
    </p>
    <ul>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/mind'>
          Mind Science</a> (a sub-category of Biology) <em>(viXra only)</em>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://arxiv.org/list/cs.DC/recent'>Distributed, Parallel,
          and Cluster Computing</a> <em>(arXiv only)</em>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/reli/'>Religion and Spiritualism</a>
        <em>(viXra only)</em>
      </li>
    </ul>
    <p>
      This provides a natural sanity check on the final models, since one naturally expects them to have an easier time
      classifying papers which
      belong to a category present in only one of the two repositories. In particular, this should provide a test
      regarding the ability of a model to discern semantic differences.
    </p>


    <h4>
      Patterns
    </h4>

    <p>
      Various patterns emerge when inspecting the data.
    </p>

    <p>
      For one, the viXra data is far more irregular:
    </p>
    <ul>
      <li>
        Many more duplicate title/abstract pairs exist on viXra.<d-footnote id='duplicates'>
          There are multiple viXra examples in which the same paper was seemingly submitted in different years, such as
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1602.0117'>this 2016</a> and
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1705.0420'>this 2017</a> submission.
          The arXiv data
          is not free of similar issues, however: for whatever reason <a target='_blank' rel='noopener noreferrer'
            href='https://arxiv.org/abs/1712.01655'>this (now-withdrawn, with comment) submission</a>
          is an exact duplicate of <a target='_blank' rel='noopener noreferrer'
            href='https://arxiv.org/abs/1703.04332'>this paper submitted earlier the same year.</a>
        </d-footnote>
      </li>
      <li>
        viXra papers are more likely to have very short or very long<d-footnote id='long-abstract'>
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1905.0204'>As in this extremely long
            viXra abstract.</a> Some outliers were also
          due to the web-scraping process in which the viXra titles and abstracts were taken directly from the paper's
          landing page. For instance,
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1702.0311'>this article's
            abstract</a> is listed simply as "1", but inspection of
          the pdf source shows that a longer abstract does indeed exist.
        </d-footnote> abstracts and titles.
      </li>
      <li>
        viXra papers are more likely<d-footnote id='common-chars'>
          For instance, taking a balanced set of training abstracts and filtering out those which have more than 3% of
          their characters outside of the set of English and Greek characters, punctuation, and digits removes
          <d-math>\mathcal{O}(2000)</d-math> viXra examples and only <d-math>\mathcal{O}(10)</d-math> arXiv ones
        </d-footnote> to be written in languages other than English.
      </li>
      <li>
        The variety of <d-code language='js'>unicode</d-code> characters among the viXra papers
        <d-footnote id='unusual-chars'>
          After forcing all text to lower-case to normalize, arXiv titles and abstracts were primarily comprised of the
          <a target='_blank' rel='noopener noreferrer' href='https://theasciicode.com.ar/'>usual 69 printable,
            non-upper-case ASCII characters</a>. In contrast,
          viXra titles and abstracts were found to use 172 and 393 distinct characters, respectively. Dead giveaways for
          viXra papers from either an algorithmic or human perspective include the use of
          <d-code language='js'>unicode</d-code> Greek characters such as ϕ or ξ or mathematical symbols such as √ or ∫
          (as
          opposed to writing these in <d-code language='python'>LaTex</d-code>).
        </d-footnote> is far wider.

      </li>
    </ul>

    <p>
      arXiv papers also tend to be much wordier. As the graphic below demonstrates, there is a clear distinction in the
      distribution of arXiv and viXra papers in terms of counting statistics such as title length or the variance in
      word
      length in their abstracts. (Based on this plot alone, one should expect that even simple algorithms may be able to
      determine the source at a reasonable rate; see <a target='_blank' rel='noopener noreferrer'
        href='https://garrettgoon.com/arxiv-vixra-baseline-models'>Baseline Models</a>.) Academics are a loquacious
      <d-footnote id='wordy'>They also tend to be long-winded, circumlocutory, logorrheic, and pleonastic.</d-footnote>
      bunch.
    </p>

    <figure style="grid-column-end: page-end" id='corner_plot'>
      <img src='images/balanced_filtered_data_df_pairplot.png' alt='Corner plot of data statistics.'>
      <figcaption>
        Some of the statistical differences between arXiv (blue) and viXra (orange) data.
        Data points are a randomly selected, equally balanced subsample of the combined dataset.
      </figcaption>
    </figure>

    <p>
      One last pattern I can't resist but mention is <a target='_blank' rel='noopener noreferrer'
        href='http://fs.unm.edu/FlorentinSmarandache.htm'>Smarandache</a>. A simple search on <a target='_blank'
        rel='noopener noreferrer' href='https://vixra.org'>viXra.org</a> for "Smarandache" yields over 6,300 results
      while the analogous search on <a target='_blank' rel='noopener noreferrer'
        href='https://arxiv.org/search/?query=Smarandache&searchtype=all&source=header'>arXiv.org</a> only gives around
      300. Simply take the <a target='_blank' rel='noopener noreferrer'
        href='https://garrettgoon.com/arxiv-vixra-quiz/'>arXiv/viXra quiz</a> for a few minutes and you will likely see
      multiple mentions of this name (or the associated phrase "Neutrosophic"). <a target='_blank'
        rel='noopener noreferrer'
        href='https://www.reddit.com/r/Physics/comments/qamty1/comment/hh5b3lp/?utm_source=share&utm_medium=web2x&context=3'>As
        this Reddit commenter</a> notes, the presence of "Smarandache" is a nearly-perfect and non-trivial predictor
      that the paper is from viXra.
    </p>

    <h3>Filtering, Normalization, and Design Choices</h3>

    <h4>Filters and Technical Markers</h4>

    <p>
      Distinguishing arXiv and viXra papers by technical clues such as the characters they use is not particularly
      interesting. Instead, being able to make the distinction based on a title or abstract's actual content (i.e.
      semantic meaning) would be much preferrable. At the same time, some technical markers (primarily the presence or
      absence of <d-code language='python'>LaTeX</d-code>) <em>do</em> carry significant meaning and, as a
      researcher, can represent obvious signals as to a paper's source.
    </p>

    <p>
      I have attempted to strike a balance between concerns like the above by filtering out papers which are very easy
      to classify
      due to reasons such as not being in English (a strong viXra signal) or containing inordinate number of uncommon
      characters. Some examples (<a target='_blank' rel='noopener noreferrer'
        href='https://github.com/garrett361/arxiv-vixra-ml/tree/main/data_exploration'>notebooks here</a>):
    </p>

    <ul>
      <li>
        Filter based on the prevalence of characters outside of the usual ASCII range.
      </li>
      <li>
        Filter extreme outliers by counting statistics, such as character length, word count, fraction of numerical
        characters, etc.
      </li>
    </ul>
    <p>
      The above steps primarily filter out viXra articles which would be fairly trivial to classify<d-footnote
        id='trivia>l-classify'>
        The examples at <a target='_blank' rel='noopener noreferrer'
          href='https://garrettgoon.com/arxiv-vixra-quiz/'>garrettgoon.com/arxiv-vixra-quiz</a> have all passed these
        series of filters. The examples were randomly chosen from the balanced training set used for all ML models.
      </d-footnote>.
    </p>

    <h4>
      Text Normalization
    </h4>

    <p>
      Properly normalizing the remaining text is important both for avoiding accidental cheating (see the next section)
      and for making a fair comparison between model and human performance
      <d-footnote id='fair'>
        Some examples: as can be seen <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml/blob/main/data_exploration/data_normalization_and_tokenization.ipynb'>in
          this notebook</a> about a third of viXra titles (and none of the arXiv titles)
        contain a
        <d-code language='python'>\r</d-code> <a target='_blank' rel='noopener noreferrer'
          href='https://stackoverflow.com/questions/1279779/what-is-the-difference-between-r-and-n'>carriage return</a>
        character, and 40% of arXiv titles (and none of the viXra titles) have a <d-code language='python'>\n</d-code>
        newline character. Neither of these technical clues are directly accessible to the human participants at <a
          target='_blank' rel='noopener noreferrer'
          href='https://garrettgoon.com/arxiv-vixra-quiz/'>garrettgoon.com/arxiv-vixra-quiz</a>, but would be obvious
        markers to an ML architecture.
      </d-footnote>.
    </p>

    <p>
      For these reasons, I performed a fairly brutal and basic normalization to the text:
    </p>
    <ul>
      <li>
        The <a target='_blank' rel='noopener noreferrer' href='https://pypi.org/project/Unidecode/'>
          <d-code language='python'>unidecode</d-code>package
        </a> was used to convert all characters to the ASCII range.
      </li>
      <li>
        All text had <d-code language='python'>.strip()</d-code> applied and all characters were forced lower-case with
        <d-code language='python'>.lower()</d-code>.
      </li>
      <li>
        Spaces were inserted around all punctuation marks and any <a target='_blank' rel='noopener noreferrer'
          href='https://en.wikipedia.org/wiki/Control_character'>ASCII control characters</a> were replaced by blank
        spaces (and any consecutive blanks were condensed into a single space).
      </li>
    </ul>


    <h4>Cautionary Tale</h4>

    <p>
      If you see a plot of validation accuracy vs time like the one below<d-footnote id='cheat'>
        This plot is from a test in which a single blank space was inserted at the end of every viXra title. After
        one-hot encoding and flattening the text
        the ensuring vector was fed into a logistic regression model which produced the below performance. This test was
        done to demonstrate the ability of even relatively simple models to pick up on technical clues.
      </d-footnote> for any non-trivial dataset, you are probably cheating. The plot correspond to achieving near
      perfect performance on the validation set after training for only a handful of epochs. Finding the cheat
      may not be easy, however.
    </p>


    <iframe
      src="https://wandb.ai/garrett361/balanced_title_one_hot_fc_cheating_test/reports/Cheating-with-Whitespace--VmlldzoxMzUwNDAx"
      style="border:none;height:1024px;width:100%">
    </iframe>

    <p>
      In the course of training recurrent architectures, I generated an even more-extreme version of the above when
      building a classifier for abstracts.
      After seeing the training examples only once, i.e., after a single epoch, the validation set accuracy was over
      99%.
    </p>

    <p>
      To make matter more confusing, the analogous models for classifying titles did not show similarly surprising
      accuracy. While I expected
      to (and eventually did) find some technical cheat as the cause, it was unclear how this could occur for the
      abstracts, but not the titles, as both texts had the same normalization and filtering procedures applied to
      them.
    </p>

    <p>
      In the end, the root cause was the fact that all raw arXiv abstracts started with a blank space and ended with a
      <d-code language='python'>\n</d-code> newline character, while none of the other datasets displayed such strong
      technical regularities. Due to a typo<d-footnote id='typo'>
        Rather than the desired <d-code language='python'>s = s.strip()</d-code> line in my code, I had accidentally had
        a simple <d-code language='python'>s.strip()</d-code>, which does nothing to modify the to-be-returned <d-code
          language='python'>s</d-code>.
      </d-footnote>, this leading and trailing whitespace was not properly stripped. Because the technical clues were in
      whitespace,
      they were difficult to detect by eye, but careful exploration of the data finally revealed the issue.
    </p>

    <h3>Final Datasets</h3>

    <p>
      After filtering, approximately 30,000 viXra and 1.7 million arXiv examples remained. In order to handle the
      massive data imbalance (and make it somewhat easier to use the data), I first created an equally balanced
      arXiv/viXra set with approximately 60,000 data points, with the unused arXiv data set aside for later use. The
      balanced dataset<d-footnote id='balance'>
        Most of the posts in this series regard models which were trained on the balanced set. A post on the utilization
        of the remaining data and handling the massive imbalance is planned for a future date.
      </d-footnote> was then split into train/val/test subsets in a 70:15:15 ratio.
    </p>

  </d-article>



  <d-appendix>


    <h3>Acknowledgments</h3>

    <p>
      Thank you to <a href="https://distill.pub" target="_blank" rel="noopener noreferrer">the <em>Distill</em> team</a>
      for making their
      <a href="https://github.com/distillpub" target="_blank" rel="noopener noreferrer">article template publicly
        available.</a> Conversations with Matt Malloy, Thomas Schaaf, and Rami Vanguri were useful in my attempts to
      find and sanity check the issue found in the "Cautionary Tale" section above.
    </p>


    <h3>Helpful Links and Resources</h3>
    <ul class="color-dot-ul">
      <li>
        I first attempted to handle the arXiv data on my laptop locally, which could not load the set into memory. <a
          target='_blank' rel='noopener noreferrer' href='https://dask.org/'>
          <d-code language='python'>dask</d-code>was very useful
        </a> for being able to lazily load the data for processing, though I eventually switched to using Colab
        notebooks whose computing power was sufficient to simply load the sets in to memory.

    </ul>

    <d-footnote-list></d-footnote-list>

    <h3>All Project Posts</h3>

    <p>Links to all posts in this series.
      <em>Note: all code for this project can be found <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml'>on my GitHub page</a>.
      </em>
    </p>

    <ul>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-data/">The Data</a>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://garrettgoon.com/arxiv-vixra-workflow/'>Workflow</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-baseline-models/">Baseline Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-recurrent/">Simple
          Recurrent Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-embeddings/">Embeddings
          (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-convolutions/">Convolutions (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-attention/">Attention (To
          Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-transformers/">Transformers (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-transfer-learning/">Transfer Learning (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-test-set-conclusions/">Test Set Performance and Conclusions (To
          Come)</a>
      </li>
    </ul>

    <d-citation-list></d-citation-list>
  </d-appendix>


  <!-- bibliography will be inlined during Distill pipeline's pre-rendering
    (GG: I have not managed to get the bibliography to compile after the ejs
     is converted to a static html file, so commenting out)
  <d-bibliography src="bibliography.bib"></d-bibliography>

   -->


</body>