<!doctype html>

<!-- Any figures to-be called with <img ...> should be placed in /static and called 
as with /static as their root. E.g. <img src="/diagrams/fig1.png">
-->

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="template.v2.js"></script>
  <title>arXiv/viXra - The Data - Garrett Goon</title>
</head>

<body>

  <!-- I added a date field to more easily add the date to the front matter. Removed the DOI fiel -->

  <d-front-matter>
    <script type="text/json">
      {
        "title": "arXiv/viXra - Data",
        "description": "An example project using Webpack, Babel, and Svelte.",
        "authors": [
          {
            "author": "Garrett Goon",
            "authorURL": "https://garrettgoon.com",
            "affiliation": "CMU",
            "affiliationURL": "https://www.cmu.edu/physics/"
          }
        ],
        "katex": {
          "delimiters": [
            {
              "left": "$",
              "right": "$",
              "display": false
            },
            {
              "left": "$$",
              "right": "$$",
              "display": true
            }
          ]
        },
        "date" : "5 November, 2021"
      }
  </script>
  </d-front-matter>

  <d-title>
    <h1>arXiv/viXra - The Data</h1>
    <p>Diving in.</p>
  </d-title>

  <d-article>

    <p>
      <em>
        <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml/tree/main/data_exploration'>Note:
          The Jupyter/Colab notebooks I
          used to perform much of the following analysis can be found on my GitHub page.</a>
      </em>
    </p>

    <h3>Wading Through</h3>

    <blockquote>
      The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly
      inspecting your data.
      This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through
      thousands of examples, understanding their distribution and looking for patterns.
    </blockquote>
    <p>
      This quote is from Andrej Karpaty's excellent post
      <em>
        <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org">A Recipe for Training Neural Networks
        </a>
      </em>, and it is advice<d-footnote id="d-footnote-wrong-guesses">
        Another sage piece of advice from the post: <em>don't be a hero.</em> Start simple and be conservative in your
        rate of adding bells and whistles.
      </d-footnote> well-taken. Below, I summarize my results in applying this advice to the arXiv/viXra
      datasets. I detail the gross properties of the sets,
      their patterns and distinctions,
      and the process of normalizing the text for the purposes of both feeding the data into ML models and preventing
      accidental cheating by
      including unwanted technical clues in the training examples. In particular, I provide a cautionary tale in which I
      failed at this final task.
    </p>



    <h3>Gross Properties</h3>

    <p>
      There is far more data<d-footnote id='data'>
        The arXiv dataset is <a target='_blank' rel='noopener noreferrer'
          href='https://www.kaggle.com/Cornell-University/arxiv'>
          publicly available on Kaggle.</a>
        I wrote a small <d-code language='python'>python</d-code> web-scraper to collect the viXra data, which can be <a
          target='_blank' rel='noopener noreferrer'
          href='https://www.dropbox.com/s/4gw9wv90kqyeh95/vixra_raw.feather?dl=0'>downloaded here</a> as a <d-code
          language='python'>.feather</d-code> file (18MB).
        <a target='_blank' rel='noopener noreferrer'
          href='https://pandas.pydata.org/docs/reference/api/pandas.read_feather.html'>
          <d-code language='python'>pandas</d-code> handles <d-code language='python'>.feather</d-code> files natively
        </a> and they are a much more efficient alternative to <d-code language='python'>.csv</d-code>.
      </d-footnote> for arXiv than viXra (the ratio of papers is 50:1) and the alignment of categories is not perfect.
      That is, while the two repositories cover many of the same topics, such as
    </p>
    <ul>
      <li>
        Number Theory
      </li>
      <li>
        Condensed Matter
      </li>
      <li>
        Data Structures and Algorithms
      </li>
    </ul>
    <p>
      there also exist categories that belong to arXiv or viXra alone:
    </p>
    <ul>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/mind'>
          Mind Science</a> (a sub-category of Biology) <em>(viXra only)</em>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://arxiv.org/list/cs.DC/recent'>Distributed, Parallel,
          and Cluster Computing</a> <em>(arXiv only)</em>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/reli/'>Religion and Spiritualism</a>
        <em>(viXra only)</em>
      </li>
    </ul>
    <p>
      This provides a natural sanity check on the final models, since one naturally expects them to have an easier time
      classifying papers which
      belong to a category present in only one of the two repositories. In particular, this should provide a test
      regarding the ability of a model to discern semantic differences.
    </p>


    <h3>Patterns</h3>

    <p>
      Various patterns emerge when inspecting the data.
    </p>

    <p>
      For one, the viXra data is far more irregular:
    </p>
    <ul>
      <li>
        Many more duplicate title/abstract pairs exist on viXra.<d-footnote id='duplicates'>
          There are multiple viXra examples in which the same paper was seemingly submitted in different years, such as
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1602.0117'>this 2016</a> and
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1705.0420'>this 2017</a> submission.
          The arXiv data
          is not free of similar issues, however: for whatever reason <a target='_blank' rel='noopener noreferrer'
            href='https://arxiv.org/abs/1712.01655'>this (now-withdrawn, with comment) submission</a>
          is an exact duplicate of <a target='_blank' rel='noopener noreferrer'
            href='https://arxiv.org/abs/1703.04332'>this paper submitted earlier the same year.</a>
        </d-footnote>
      </li>
      <li>
        viXra papers are more likely to have very short or very long<d-footnote id='long-abstract'>
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1905.0204'>As in this extremely long
            viXra abstract.</a> Some outliers were also
          due to the web-scraping process in which the viXra titles and abstracts were taken directly from the paper's
          landing page. For instance,
          <a target='_blank' rel='noopener noreferrer' href='https://vixra.org/abs/1702.0311'>this article's
            abstract</a> is listed simply as "1", but inspection of
          the pdf source shows that a longer abstract does indeed exist.
        </d-footnote> abstracts and titles.
      </li>
      <li>
        viXra papers are more likely<d-footnote id='common-chars'>
          For instance, taking a balanced set of training abstracts and filtering out those which have more than 3% of
          their characters outside of the set of English and Greek characters, punctuation, and digits removes
          <d-math>\mathcal{O}(2000)</d-math> viXra examples and only <d-math>\mathcal{O}(10)</d-math> arXiv ones
        </d-footnote> to be written in languages other than English.
      </li>
      <li>
        The variety of <d-code language='js'>unicode</d-code> characters among the viXra papers
        <d-footnote id='unusual-chars'>
          After forcing all text to lower-case to normalize, arXiv titles and abstracts were primarily comprised of the
          <a target='_blank' rel='noopener noreferrer' href='https://theasciicode.com.ar/'>usual 69 printable,
            non-upper-case ASCII characters</a>. In contrast,
          viXra titles and abstracts were found to use 172 and 393 distinct characters, respectively. Dead giveaways for
          viXra papers from either an algorithmic or human perspective include the use of
          <d-code language='js'>unicode</d-code> Greek characters such as ϕ or ξ or mathematical symbols such as √ or ∫
          (as
          opposed to writing these in <d-code language='python'>LaTex</d-code>).
        </d-footnote> is far wider.

      </li>
    </ul>

    <p>
      arXiv papers also tend to be much wordier. As the graphic below demonstrates, there is a clear distinction in the
      distribution of arXiv and viXra papers in terms of counting statistics such as title length or the variance in
      word
      length in their abstracts. (Based on this plot alone, one should expect that even simple algorithms may be able to
      determine the source at a reasonable rate; see <a target='_blank' rel='noopener noreferrer'
        href='https://garrettgoon.com/arxiv-vixra-baseline-models'>Baseline Models</a>.) Academics are a loquacious
      <d-footnote id='wordy'>They also tend to be long-winded, circumlocutory, logorrheic, and pleonastic.</d-footnote>
      bunch.
    </p>

    <d-figure style="grid-column-end: page-end">
      <img src='images/balanced_filtered_data_df_pairplot.png' alt='Corner plot of data statistics.'>
      <figcaption>
        Some of the statistical differences between arXiv (blue) and viXra (orange) data.
        Data points are a randomly selected, equally balanced subsample of the combined dataset.
      </figcaption>
    </figure>

    <p>
      One last pattern I can't resist but mention is <a target='_blank' rel='noopener noreferrer'
        href='http://fs.unm.edu/FlorentinSmarandache.htm'>Smarandache</a>. A simple search on <a target='_blank'
        rel='noopener noreferrer' href='https://vixra.org'>viXra.org</a> for "Smarandache" yields over 6,300 results
      while the analogous search on <a target='_blank' rel='noopener noreferrer'
        href='https://arxiv.org/search/?query=Smarandache&searchtype=all&source=header'>arXiv.org</a> only gives around
      300. Simply take the <a target='_blank' rel='noopener noreferrer'
        href='https://garrettgoon.com/arxiv-vixra-quiz/'>arXiv/viXra quiz</a> for a few minutes and you will likely see
      multiple mentions of this name (or the associated phrase "Neutrosophic"). <a target='_blank'
        rel='noopener noreferrer'
        href='https://www.reddit.com/r/Physics/comments/qamty1/comment/hh5b3lp/?utm_source=share&utm_medium=web2x&context=3'>As
        this Reddit commenter</a> notes, the presence of "Smarandache" is a nearly-perfect and non-trivial predictor
      that the paper is from viXra.
    </p>

    <h3>Filtering</h3>

    <p>
      Distinguishing arXiv and viXra papers by technical clues such as the characters they use is not particularly
      interesting. Instead, being able to make the distinction based on a title or abstract's actual content (i.e.
      semantic meaning) would be much preferrable. At the same time, some technical markers (primarily the presence or
      absence of <d-code language='python'>LaTeX</d-code>) <em>do</em> carry significant stylistic meaning and, as a
      researcher, can represent obvious signals as to a paper's source.
    </p>

    <p>
      For the above reasons, I apply filters such as the following to the data (<a target='_blank'
        rel='noopener noreferrer'
        href='https://github.com/garrett361/arxiv-vixra-ml/tree/main/data_exploration'>notebooks here</a>):
    </p>

    <ul>
      <li>
        Filter based on the prevalence of characters outside of the usual ASCII range.
      </li>
      <li>
        Filter extreme outliers by counting statistics, such as character length, word count, fraction of numerical
        characters, etc.
      </li>
    </ul>
    <p>
      The above steps primarily filter out viXra articles which would be fairly trivial to classify and are hence not
      particularly interesting for this project.
    </p>

    <h3>
      Text Normalization
    </h3>

    <p>
      Properly normalizing the remaining text is important both for avoiding accidental cheating (see the next section)
      and for making a fair comparison between model and human performance
      <d-footnote id='fair'>
        Some examples: as can be seen <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml/blob/main/data_exploration/data_normalization_and_tokenization.ipynb'>in
          this notebook</a> about a third of viXra titles (and none of the arXiv titles)
        contain a
        <d-code language='python'>\r</d-code> <a target='_blank' rel='noopener noreferrer'
          href='https://stackoverflow.com/questions/1279779/what-is-the-difference-between-r-and-n'>carriage return</a>
        character, and 40% of arXiv titles (and none of the viXra titles) have a <d-code language='python'>\n</d-code>
        newline character. Neither of these technical clues are accessible to the human participants at <a
          target='_blank' rel='noopener noreferrer'
          href='https://garrettgoon.com/arxiv-vixra-quiz/'>garrettgoon.com/arxiv-vixra-quiz</a>.
      </d-footnote>.
    </p>

    <p>
      For these reasons, I performed a fairly brutal and basic normalization to the text:
    </p>
    <ul>
      <li>
        The <a target='_blank' rel='noopener noreferrer' href='https://pypi.org/project/Unidecode/'>
          <d-code language='python'>unidecode</d-code>package
        </a> was used to convert all characters to the ASCII range.
      </li>
      <li>
        All text had <d-code language='python'>.strip()</d-code> applied and all characters were forced lower-case with
        <d-code language='python'>.lower()</d-code>.
      </li>
      <li>
        Spaces were inserted around all punctuation marks and any <a target='_blank' rel='noopener noreferrer'
          href='https://en.wikipedia.org/wiki/Control_character'>ASCII control characters</a> were replaced by blank
        spaces (and any consecutive blanks were condensed into a single space).
      </li>
    </ul>

    <h3>Cautionary Tale</h3>

    <p>
      If you see a plot of validation accuracy vs time like the one below<d-footnote id='cheat'>
        This plot was from a test in which a single blank space was inserted at the end of every viXra title. A simple
        linear model was used in the test: one-hot encoded text was flattened into a vector and passed through a single
        <d-code language='python'>nn.Linear</d-code> layer and sigmoid function, i.e. the arXiv/viXra probability for a
        text vector <d-math>\vec{t}</d-math> was given by <d-math>s = \sigma\left(W\cdot \vec{t}+b\right)</d-math>. In
        other words, logistic regression. The actual cheat described here was found when testing recurrent architectures and the corresponding plot was even
        more dramatic. This test was done to demonstrate the ability of even relatively simple models to pick up on
        technical biases.
      </d-footnote> for any non-trivial dataset, you are probably cheating. The plot correspond to achieving near 100%
      validation-set accuracy after only a handful of epochs. Finding the cheat
      may not be easy, however.
    </p>


    <iframe
      src="https://wandb.ai/garrett361/balanced_title_one_hot_fc_cheating_test/reports/Cheating-with-Whitespace--VmlldzoxMzUwNDAx"
      style="border:none;height:1024px;width:100%">
    </iframe>

    <p>
      In the course of testing recurrent architectures, I generate a more-extreme version of the above when training
      on
      abstracts. The analogous plots for the title data did not show similarly surprising accuracy. While I expected
      to
      (and eventually did) find some technical cheat as the cause, it was quite confusing how this could occur for the
      abstracts, but not the titles, as both texts had the same normalization and filtering procedures applied to
      them.
    </p>

    <p>
      In the end, the root cause was the fact that all raw arXiv abstracts started with a blank space and ends with a
      <d-code language='python'>\n</d-code> newline character, while none of the other datasets displayed similar
      technical regularities. Due to a typo<d-footnote id='typo'>
        I had a simple <d-code language='python'>s.strip()</d-code> line in my code, rather than the desired <d-code
          language='python'>s = s.strip()</d-code>.
      </d-footnote>
      this leading and trailing whitespace was not properly stripped. Because the technical clues were in whitespace,
      they were difficult to detect by eye, but a systematic exploration of the data revealed the issue.
    </p>

    <h3>Final Datasets</h3>

    <p>
      After filtering, approximately 30,000 viXra and 1.7 million arXiv examples remained. In order to handle the
      massive data imbalance (and make it somewhat easier to use the data), I first created an equally balanced
      arXiv/viXra set with approximately 60,000 data points, with the unused arXiv data set aside for later use. The
      balanced dataset<d-footnote id='balance'>
        The balanced set was used for the current set of posts in this series. A post using the full dataset with a
        discussion of how to handle the data imbalanced in planned for a future date.
      </d-footnote> was then split into train/val/test subsets in a 70:15:15 ratio.
    </p>

  </d-article>



  <d-appendix>


    <h3>Acknowledgments</h3>

    <p>
      Thank you to <a href="https://distill.pub" target="_blank" rel="noopener noreferrer">the <em>Distill</em> team</a>
      for making their
      <a href="https://github.com/distillpub" target="_blank" rel="noopener noreferrer">article template publicly
        available.</a> Conversations with Matt Malloy, Thomas Schaaf, and Rami Vanguri were useful in my attempts to
      find and sanity check the issue found in the "Cautionary Tale" section above.
    </p>


    <h3>Helpful Links and Resources</h3>
    <ul class="color-dot-ul">
      <li>
        I first attempted to handle the arXiv data on my laptop locally, which could not load the set into memory. <a
          target='_blank' rel='noopener noreferrer' href='https://dask.org/'>
          <d-code language='python'>dask</d-code>was very useful
        </a> for being able to lazily load the data for processing, though I eventually switched to using Colab
        notebooks whose computing power was sufficient to simply load the sets in to memory.

    </ul>

    <d-footnote-list></d-footnote-list>

    <h3>All Project Posts</h3>

    <p>Links to all posts for this work.
      <em>Note: all code for this project can be found <a target='_blank' rel='noopener noreferrer'
          href='https://github.com/garrett361/arxiv-vixra-ml'>on my GitHub page</a>.
      </em>
    </p>

    <ul>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-data/">The Data</a>
      </li>
      <li>
        <a target='_blank' rel='noopener noreferrer' href='https://garrettgoon.com/arxiv-vixra-workflow/'>Workflow</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-baseline-models/">Baseline Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-recurrent/">Simple
          Recurrent Models</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-embeddings/">Embeddings
          (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-convolutions/">Convolutions (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer" href="https://garrettgoon.com/arxiv-vixra-attention/">Attention (To
          Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-transformers/">Transformers (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-transfer-learning/">Transfer Learning (To Come)</a>
      </li>
      <li>
        <a target="_blank" rel="noopener noreferrer"
          href="https://garrettgoon.com/arxiv-vixra-test-set-conclusions/">Test Set Performance and Conclusions (To
          Come)</a>
      </li>
    </ul>

    <d-citation-list></d-citation-list>
  </d-appendix>


  <!-- bibliography will be inlined during Distill pipeline's pre-rendering
    (GG: I have not managed to get the bibliography to compile after the ejs
     is converted to a static html file, so commenting out)
  <d-bibliography src="bibliography.bib"></d-bibliography>

   -->


</body>